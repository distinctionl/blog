<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[The Distinctionl Blog]]></title>
  <link href="http://blog.distinctionl.com/atom.xml" rel="self"/>
  <link href="http://blog.distinctionl.com/"/>
  <updated>2014-09-27T11:15:37+01:00</updated>
  <id>http://blog.distinctionl.com/</id>
  <author>
    <name><![CDATA[Oliver Woodford]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The perils of overfitting]]></title>
    <link href="http://blog.distinctionl.com/blog/2014/09/27/the-perils-of-overfitting/"/>
    <updated>2014-09-27T10:59:37+01:00</updated>
    <id>http://blog.distinctionl.com/blog/2014/09/27/the-perils-of-overfitting</id>
    <content type="html"><![CDATA[<p>Given my last blog post on using learning curves to diagnose over or under-fitting, it&rsquo;s a fitting ;) time to share this video. In it some talented folks from Brown University and Georgia Tech lay bare the perils of both over and under-fitting, in a rather original way. So without further ado, take it away guys&hellip;</p>

<iframe width="640" height="360" src="//www.youtube.com/embed/fJMXDlNkYvU?rel=0" frameborder="0" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analysing learning curves]]></title>
    <link href="http://blog.distinctionl.com/blog/2014/09/22/analysing-learning-curves/"/>
    <updated>2014-09-22T17:02:38+01:00</updated>
    <id>http://blog.distinctionl.com/blog/2014/09/22/analysing-learning-curves</id>
    <content type="html"><![CDATA[<p>I recently looked over Machine Learning high-flier <a href="http://en.wikipedia.org/wiki/Andrew_Ng">Andrew Ng</a>&rsquo;s <a href="http://www.coursera.org/course/ml">Machine Learning course</a> on <a href="http://www.coursera.org/">Coursera</a>. It&rsquo;s perfect for people who want to use machine learning as a tool, and is also a great primer for people who want to get into machine learning research, though it is far more vocational than theoretical.</p>

<p>Of course, being practical, it has all sorts of useful tips to help you get your methods working well. One such tip is some basic plotting and analysis of the <em>learning curves</em> of your current implementation, which can point you towards what to try next if things aren&rsquo;t quite working yet. This analysis can save you a lot of time! I recommend the course, but if you just want to know how to do this analysis then look no further than this blog post.</p>

<!-- more -->


<h2>Data, model and learning</h2>

<p>For any machine learning problem, you should have some training data, and a model you want to learn, or fit to the data. For example, in this post I&rsquo;ll be using a dataset of <a href="http://people.sc.fsu.edu/~jburkardt/datasets/regression/x17.txt">fuel octane rating versus raw materials</a>, and the model I&rsquo;ll be fitting to this data is a linear function which takes as input the raw material composition, and outputs the real-valued octane rating - this is known as linear regression.</p>

<p>The model defines a cost function to minimize. This generally consists of a term which indicates how well the model agrees with the data, called the <em>data cost</em>, and possibly one or more other terms, which can regularize the space of solutions. Learning is then the process of minimizing the cost function over the training set, with respect to the model parameters. All very standard thus far. In addition, you need to split your data up into two sets, one for learning the model parameters (the &ldquo;training&rdquo; set), and one for evaluating them (the &ldquo;test&rdquo; set). That&rsquo;s fairly common too.</p>

<h2>Learning curves</h2>

<p>Learning curves are created as follows:</p>

<ol>
<li>Learn the model parameters for several different sizes of training data sets, upto the largest size possible (whilst leaving enough data aside for the test set).</li>
<li>For each set of learned parameters, compute the average data cost (note that this does not include any regularization costs) per input datum used to learn the parameters, and per test set datum.</li>
<li>Plot the average data costs for both the training set and the test set, plotted against the training set size.</li>
</ol>


<p>I have done this below, using the example dataset I mentioned earlier, for two cases, the first using just one material composition value (or data feature), and the second using all four values (or features) given in the dataset.</p>

<p><img src="http://blog.distinctionl.com/images/learning_curve_1.png" alt="" />
<img src="http://blog.distinctionl.com/images/learning_curve_2.png" alt="" /></p>

<p>What the graphs show is that at a certain training set size, the average data costs converge on a stable value, which is the same for the training and test sets. They also show that the learning curves for the case of just one feature have a significantly higher data cost than the other case.</p>

<h2>Bias and variance</h2>

<p>Bias is the name given to a constant error, in the case of learning curves an error which affects both the training and test sets. The first graph above exhibits high bias, because both curves settle on a large average data cost, relative to what we might expect or desire. High bias is caused by the model underfitting the data; the particular bias above caused by the model not having enough features to fit to. By contrast, the second graph, where the model has more features to use, has a lower bias, within the realms of what one might wish for.</p>

<p>Variance is the name given to a variable error, in the case of learning curves an error which affects the test set but not the training set. Below I have plotted two further pairs of learning curves. In the case of the first graph I have used even more features, by including all quadratic combinations of features, giving twenty features in total. What you can see is that the data costs do not quite stabilize as the training set size increases, though they look like they could converge with more data. In addition, the data cost of the test set is significantly higher than that of the training set, so this graph exhibits high variance.</p>

<p><img src="http://blog.distinctionl.com/images/learning_curve_3.png" alt="" />
<img src="http://blog.distinctionl.com/images/learning_curve_4.png" alt="" /></p>

<p>High variance is caused by the model overfitting the training data, and therefore not transferring well to the test data. Overfitting can be caused by having many unnecessary features (as is likely in the case above), or not enough training data. One way of reducing overfitting is to use a regularization term in the cost function, which prefers simpler models. I have used a quadratic cost on the model parameters to regularize the model, whose learning curves are shown in the second graph above. This graph exhibits low variance, indicating that the model is now not overfit.</p>

<h2>What steps to take</h2>

<p>When you look at your learning curves, you should see one of three scenarios:</p>

<ol>
<li>The test set data cost is higher than the training set data cost (high variance), indicating that the model is overfitting the data. Possible steps to try in this case are:

<ol>
<li>Using more training data.</li>
<li>Using fewer data features.</li>
<li>Using a regularization term which prefers simpler models.</li>
</ol>
</li>
<li>The data costs are similar, but higher than desirable (high bias), indicating that the model is underfitting the data. Possible steps to try in this case are:

<ol>
<li>Using more data features, either by taking more measurements or by creating features by multiplying together those already given.</li>
<li>Using a more complex model, e.g. by changing from linear regression to a neural net.</li>
</ol>
</li>
<li>The data costs are similar, and sufficiently low, indicating low variance and low bias. In this case you&rsquo;re all done!</li>
</ol>


<p>I hope that is useful. Code for generating these figures can be found in <em>bias_variance_analysis_demo.m</em> from our <a href="http://github.com/distinctionl/ml_examples">ml_examples GitHub repository</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Get lost in the cloud]]></title>
    <link href="http://blog.distinctionl.com/blog/2014/09/08/get-lost-in-the-cloud/"/>
    <updated>2014-09-08T14:05:42+01:00</updated>
    <id>http://blog.distinctionl.com/blog/2014/09/08/get-lost-in-the-cloud</id>
    <content type="html"><![CDATA[<p>Hopefully you find <a href="http://www.ted.com">TED talks</a> as inspiring and thought provoking as I do. Anyway, I came across a real gem for all researchers the other day. Now this is nothing to do with cloud storage or cloud computing. It posits the idea that in order to have a great new idea, you have to get lost and confused. Mindblowingly simple, yet insightful. Enjoy&hellip;</p>

<iframe src="https://embed-ssl.ted.com/talks/uri_alon_why_truly_innovative_science_demands_a_leap_into_the_unknown.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Now get lost.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DeepMind patents: I]]></title>
    <link href="http://blog.distinctionl.com/blog/2014/09/02/deepmind-patents-i/"/>
    <updated>2014-09-02T14:12:01+01:00</updated>
    <id>http://blog.distinctionl.com/blog/2014/09/02/deepmind-patents-i</id>
    <content type="html"><![CDATA[<p>I&rsquo;m sure many of you will have heard about <a href="http://www.bbc.co.uk/news/technology-25908379">Google&rsquo;s Â£400m purchase</a> of <a href="http://deepmind.com">DeepMind</a>, a startup founded to solve Artificial Intelligence (AI). They focussed on developing algorithms that would allow machines to learn how to play computer games. It&rsquo;s this generality which makes their approach AI rather than Machine Learning (ML). And apparently the computer gets pretty good, eventually beating human players.</p>

<p>What intrigues me is how this technology works. Of course much of it is a closely guarded secret, but prior to being bought, DeepMind did file three patents, so I thought it would be interesting to read these. In this blog post, I&rsquo;ll be taking a look at the first of them, <a href="http://www.google.com/patents/US20140185959">US20140185959</a>.</p>

<!-- more -->


<p>To save some of you some time, it&rsquo;s probably worth saying up front that this particular patent doesn&rsquo;t contain any AI or ML technology. Instead, it presents two ideas, to be used in conjunction, for the purpose of searching for images using example images, a.k.a. <em>content-based image retrieval</em>.</p>

<p>The first idea is what I&rsquo;ll call the <em>composite feature query</em>. Basically, it&rsquo;s an image query that consists of user-selected image features from one or more images. For example, if you wanted to find a particular sofa you could use the shape and colour features of a sofa in one image with the texture of some curtains in another image as your query.</p>

<p>The second idea is what I&rsquo;ll call <em>iterated query refinement</em>. You start with one or more features from one image, do a search, then use one or more extra features from the original image or the search results to augment the query, and repeat the search. This process can be repeated until you find an image you&rsquo;re happy with.</p>

<p>That&rsquo;s it! It&rsquo;s not related to learning to play computer games. But then, that could be just the toy problem that DeepMind use to develop their algorithms, and perhaps making online shopping easier is the real target application (or at least one of them). Nor does it explain how image features might be described and matched within the search process - the tricky ML part. Perhaps the answer to that will crop up in other patents. But I am reminded of something my friend Tom at <a href="http://metail.com/">Metail</a> once told me, which was that if you&rsquo;re a business that&rsquo;s looking to get bought, then you need an IP portfolio that&rsquo;s broad, covering all areas of your business, not just the core technology.</p>
]]></content>
  </entry>
  
</feed>
